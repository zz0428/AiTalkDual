import ollama
import time
import sys

# --- é…ç½®åŒº ---
# ä½ å¯ä»¥åœ¨è¿™é‡Œæ›´æ¢ä½ å·²ç»ä¸‹è½½å¥½çš„ä»»ä½•æ¨¡å‹
MODEL_1_NAME = 'qwen2:7b'
MODEL_2_NAME = 'deepseek-r1:latest'

# å¯¹è¯çš„åˆå§‹è¯é¢˜
STARTING_PROMPT = "ä½ å¥½ï¼Œæˆ‘ä»¬æ¥ç©ä¸€ä¸ªè§’è‰²æ‰®æ¼”æ¸¸æˆã€‚ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å®‡èˆªå‘˜ï¼Œæˆ‘æ˜¯ä¸€ä¸ªå¯¹å¤ªç©ºå……æ»¡å¥½å¥‡çš„é«˜ä¸­ç”Ÿã€‚è¯·ä½ å…ˆå¼€å§‹ï¼Œå‘æˆ‘ä»‹ç»ä¸€ä¸‹ä½ ç¬¬ä¸€æ¬¡è¿›å…¥å¤ªç©ºæ—¶çš„æ„Ÿå—ã€‚"

# å¯¹è¯è¿›è¡Œçš„è½®æ•°
CONVERSATION_TURNS = 4

# æ‰“å­—æœºæ•ˆæœçš„é€Ÿåº¦ï¼ˆç§’/å­—ç¬¦ï¼‰ï¼Œæ•°å€¼è¶Šå°é€Ÿåº¦è¶Šå¿«
TYPEWRITER_SPEED = 0.03 
# --- é…ç½®åŒºç»“æŸ ---


def stream_to_terminal(text, speed):
    """
    ä»¥æ‰“å­—æœºæ•ˆæœå°†æ–‡æœ¬é€å­—è¾“å‡ºåˆ°ç»ˆç«¯ã€‚
    """
    for char in text:
        sys.stdout.write(char)
        sys.stdout.flush()
        time.sleep(speed)
    print() # æ‰“å°æ¢è¡Œç¬¦


def main():
    """
    ä¸»å‡½æ•°ï¼Œè¿è¡Œä¸¤ä¸ªæ¨¡å‹çš„å¯¹è¯ã€‚
    """
    print("="*50)
    print(f"ğŸ¤– æ¨¡å‹1: {MODEL_1_NAME}")
    print(f"ğŸ¤– æ¨¡å‹2: {MODEL_2_NAME}")
    print(f"ğŸ’¬ åˆå§‹è¯é¢˜: {STARTING_PROMPT}")
    print("="*50)
    print("\nå¯¹è¯å¼€å§‹...\n")
    time.sleep(2)

    # ä¸ºæ¯ä¸ªæ¨¡å‹ç»´æŠ¤ç‹¬ç«‹çš„å¯¹è¯å†å²ï¼Œè¿™å¯¹äºä¿æŒä¸Šä¸‹æ–‡è‡³å…³é‡è¦
    model_1_messages = []
    model_2_messages = []

    # ä½¿ç”¨åˆå§‹è¯é¢˜å¯åŠ¨å¯¹è¯
    # æ¨¡å‹1ä½œä¸ºå¯¹è¯çš„å‘èµ·è€…
    current_prompt = STARTING_PROMPT
    
    try:
        # è¿›è¡ŒæŒ‡å®šè½®æ•°çš„å¯¹è¯
        for i in range(CONVERSATION_TURNS):
            
            # --- æ¨¡å‹1çš„å›åˆ ---
            print(f"\n--- ç¬¬ {i+1} è½® | {MODEL_1_NAME} æ­£åœ¨æ€è€ƒ... ---\n")
            stream_to_terminal(f"ğŸ‘¨â€ğŸš€ {MODEL_1_NAME}:", TYPEWRITER_SPEED)
            
            # å°†å½“å‰æç¤ºåŠ å…¥æ¨¡å‹1çš„å†å²è®°å½•
            model_1_messages.append({'role': 'user', 'content': current_prompt})
            
            # è°ƒç”¨Ollama API
            response_1 = ollama.chat(model=MODEL_1_NAME, messages=model_1_messages)
            response_1_content = response_1['message']['content']
            
            # ä»¥æ‰“å­—æœºæ•ˆæœæ˜¾ç¤ºå›åº”
            stream_to_terminal(response_1_content, TYPEWRITER_SPEED)
            
            # å°†æ¨¡å‹1è‡ªå·±çš„å›ç­”åŠ å…¥å…¶å†å²ï¼ˆä½œä¸º'assistant'è§’è‰²ï¼‰
            model_1_messages.append({'role': 'assistant', 'content': response_1_content})
            
            # å°†æ¨¡å‹1çš„å›åº”ä½œä¸ºç»™æ¨¡å‹2çš„æ–°æç¤º
            current_prompt = response_1_content
            
            time.sleep(1) # åœ¨æ¨¡å‹åˆ‡æ¢é—´ç¨ä½œåœé¡¿

            # --- æ¨¡å‹2çš„å›åˆ ---
            print(f"\n--- ç¬¬ {i+1} è½® | {MODEL_2_NAME} æ­£åœ¨æ€è€ƒ... ---\n")
            stream_to_terminal(f"ğŸ§‘â€ğŸ“ {MODEL_2_NAME}:", TYPEWRITER_SPEED)
            
            # å°†å½“å‰æç¤ºåŠ å…¥æ¨¡å‹2çš„å†å²è®°å½•
            model_2_messages.append({'role': 'user', 'content': current_prompt})
            
            # è°ƒç”¨Ollama API
            response_2 = ollama.chat(model=MODEL_2_NAME, messages=model_2_messages)
            response_2_content = response_2['message']['content']

            # ä»¥æ‰“å­—æœºæ•ˆæœæ˜¾ç¤ºå›åº”
            stream_to_terminal(response_2_content, TYPEWRITER_SPEED)

            # å°†æ¨¡å‹2è‡ªå·±çš„å›ç­”åŠ å…¥å…¶å†å²
            model_2_messages.append({'role': 'assistant', 'content': response_2_content})

            # å°†æ¨¡å‹2çš„å›åº”ä½œä¸ºç»™æ¨¡å‹1çš„æ–°æç¤º
            current_prompt = response_2_content
            
            time.sleep(1)

    except Exception as e:
        print(f"\n\nç¨‹åºå‡ºé”™: {e}")
        print("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æ¨¡å‹åç§°æ­£ç¡®ã€‚")

    print("\n\n" + "="*50)
    print("å¯¹è¯ç»“æŸã€‚")
    print("="*50)


if __name__ == "__main__":
    main()