import ollama
import time
import sys

# --- Improved Configuration ---
# Separate context prompts for each model - they don't know about each other!

# Model 1: Gets a natural conversation starter without knowing it's talking to AI
MODEL_1_NAME = 'qwen2:1.5b'
MODEL_1_CONTEXT = """You are an experienced astronaut who has just returned from a mission to the International Space Station. You're feeling excited and want to share your experiences with someone. Start by telling them about the most amazing moment during your recent space mission."""

# Model 2: Gets a different context that makes them seem like a curious person
MODEL_2_NAME = 'llama3.2:1b'
MODEL_2_CONTEXT = """You are a curious high school student who is fascinated by space and science. You just met someone who seems to have interesting stories about space. You're eager to learn and ask thoughtful questions about their experiences."""

# Conversation settings
CONVERSATION_TURNS = 4
TYPEWRITER_SPEED = 0.05
# --- Configuration End ---


def stream_to_terminal(text, speed):
    """ä»¥æ‰“å­—æœºæ•ˆæœå°†æ–‡æœ¬é€å­—è¾“å‡ºåˆ°ç»ˆç«¯"""
    for char in text:
        sys.stdout.write(char)
        sys.stdout.flush()
        time.sleep(speed)
    print()


def initialize_model_context(model_name, context_prompt):
    """Initialize a model with its own private context without revealing the conversation structure"""
    messages = [{'role': 'system', 'content': context_prompt}]
    
    # Get the model's natural opening based on their context
    response = ollama.chat(model=model_name, messages=messages)
    opening_message = response['message']['content']
    
    # Update history to include the opening
    messages.append({'role': 'assistant', 'content': opening_message})
    
    return messages, opening_message


def main():
    """è¿è¡Œæ”¹è¿›çš„åŒAIå¯¹è¯ï¼Œä¸è®©æ¨¡å‹çŸ¥é“å¯¹æ–¹æ˜¯AI"""
    print("="*50)
    print(f"ğŸ¤– æ¨¡å‹1: {MODEL_1_NAME} (Astronaut)")
    print(f"ğŸ¤– æ¨¡å‹2: {MODEL_2_NAME} (Student)")
    print("ğŸ’¬ Independent contexts - models don't know they're talking to AI!")
    print("="*50)
    print("\nåˆå§‹åŒ–æ¨¡å‹ä¸Šä¸‹æ–‡...\n")
    
    try:
        # Initialize each model with their own private context
        print("Setting up Model 1 context...")
        model_1_messages, model_1_opening = initialize_model_context(MODEL_1_NAME, MODEL_1_CONTEXT)
        
        print("Setting up Model 2 context...")
        model_2_messages, _ = initialize_model_context(MODEL_2_NAME, MODEL_2_CONTEXT)
        
        print("\nå¯¹è¯å¼€å§‹...\n")
        time.sleep(2)
        
        # Start conversation with Model 1's natural opening
        print(f"--- ç¬¬ 1 è½® | {MODEL_1_NAME} å¼€å§‹å¯¹è¯ ---\n")
        stream_to_terminal(f"ğŸ‘¨â€ğŸš€ {MODEL_1_NAME}: {model_1_opening}", TYPEWRITER_SPEED)
        
        # This becomes the first message for Model 2 (without revealing it came from AI)
        current_prompt = model_1_opening
        
        # Continue conversation
        for i in range(CONVERSATION_TURNS):
            time.sleep(1)
            
            # --- Model 2's turn ---
            print(f"\n--- ç¬¬ {i+1} è½® | {MODEL_2_NAME} æ­£åœ¨æ€è€ƒ... ---\n")
            stream_to_terminal(f"ğŸ§‘â€ğŸ“ {MODEL_2_NAME}:", TYPEWRITER_SPEED)
            
            # Model 2 receives the message as if from a human conversation partner
            model_2_messages.append({'role': 'user', 'content': current_prompt})
            
            response_2 = ollama.chat(model=MODEL_2_NAME, messages=model_2_messages)
            response_2_content = response_2['message']['content']
            
            stream_to_terminal(response_2_content, TYPEWRITER_SPEED)
            model_2_messages.append({'role': 'assistant', 'content': response_2_content})
            
            current_prompt = response_2_content
            time.sleep(1)
            
            if i < CONVERSATION_TURNS - 1:  # Don't do Model 1's turn on the last iteration
                # --- Model 1's turn ---
                print(f"\n--- ç¬¬ {i+2} è½® | {MODEL_1_NAME} æ­£åœ¨æ€è€ƒ... ---\n")
                stream_to_terminal(f"ğŸ‘¨â€ğŸš€ {MODEL_1_NAME}:", TYPEWRITER_SPEED)
                
                # Model 1 receives the message as if from a human conversation partner
                model_1_messages.append({'role': 'user', 'content': current_prompt})
                
                response_1 = ollama.chat(model=MODEL_1_NAME, messages=model_1_messages)
                response_1_content = response_1['message']['content']
                
                stream_to_terminal(response_1_content, TYPEWRITER_SPEED)
                model_1_messages.append({'role': 'assistant', 'content': response_1_content})
                
                current_prompt = response_1_content

    except Exception as e:
        print(f"\n\nç¨‹åºå‡ºé”™: {e}")
        print("è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æ¨¡å‹åç§°æ­£ç¡®ã€‚")

    print("\n\n" + "="*50)
    print("å¯¹è¯ç»“æŸã€‚")
    print("="*50)


if __name__ == "__main__":
    main()